{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMqyQvqN3Xw5ViU6SnwPI9J"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "reh9g-Fxthvy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1914dfbe-72d5-474c-d6a4-c60162374b78"
      },
      "source": [
        "from os import listdir\n",
        "import string\n",
        "from pickle import dump\n",
        "\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.models import Model"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b08WaooStnMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# feature extraction\n",
        "def feature_extraction(directory):\n",
        "\tmodel = VGG16()\n",
        " \t# model = VGG19()\n",
        "\t# model = InceptionV3()\n",
        "\t# model = ResNet50()\n",
        "\t# model = InceptionResNetV2()\n",
        "\t# feature_extractor = InceptionResNetV2(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
        "\t# model = feature_extractor.output\n",
        "\t# model = GlobalAveragePooling2D()(model)\n",
        "\t# model = Dropout(0.5)(model)\n",
        "\t# model = Dense(4096, activation=\"relu\")(model)\n",
        "\t# model = Dropout(0.5)(model)\n",
        "\t# model = Dense(4096, activation=\"relu\")(model)\n",
        "\n",
        "\tmodel.layers.pop()\n",
        "\tmodel = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
        "\n",
        "\tfeatures = dict()\n",
        "\tfor name in listdir(directory):\n",
        "\t\timg = load_img(directory + '/' + name, target_size=(224, 224))\n",
        "\t\timg = img_to_array(img)\n",
        "\t\timg = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))\n",
        "\t\timg_id = name.split('.')[0]\n",
        "\t\tfeatures[img_id] = model.predict(preprocess_input(img), verbose=0)\n",
        "\treturn features\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yC5mMoJktqCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extracting features and dumping the features into a pickle file\n",
        "directory = 'Flicker8k_Dataset'\n",
        "features = feature_extraction(directory)\n",
        "print('Number of Features Extracted:', len(features))\n",
        "dump(features, open('features.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg9ENkEZeRdq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#extracting and cleaning up the descriptions\n",
        "def load_file(name):\n",
        "\tf = open(name, 'r')\n",
        "\ttxt = f.read()\n",
        "\tf.close()\n",
        "\treturn txt\n",
        "\n",
        "\n",
        "def extract_descriptions(file):\n",
        "\td_map = {}\n",
        "\tfor r in file.split('\\n'):\n",
        "\t\tword = r.split()\n",
        "\t\tif len(r) < 2:\n",
        "\t\t\tcontinue\n",
        "\t\timg_id, img_desc = word[0], word[1:]\n",
        "\t\timg_id = img_id.split('.')[0]\n",
        "\t\timg_desc = ' '.join(img_desc)\n",
        "\t\tif img_id not in d_map:\n",
        "\t\t\td_map[img_id] = []\n",
        "\t\td_map[img_id].append(img_desc)\n",
        "\treturn d_map\n",
        "\n",
        "def get_vocab(text):\n",
        "\tdoc = set()\n",
        "\tfor k in text.keys():\n",
        "\t\t[doc.update(txt.split()) for txt in text[k]]\n",
        "\treturn doc\n",
        "\n",
        "def proc_descriptions(text):\n",
        "\tlemma = str.maketrans('', '', string.punctuation)\n",
        "\tfor key, desc_list in text.items():\n",
        "\t\tfor word in range(len(desc_list)):\n",
        "\t\t\tdesc = desc_list[word].split()\n",
        "\t\t\tdesc = [w for w in desc if len(w)>1]\n",
        "\t\t\tdesc = [w.lower() for w in desc]\n",
        "\t\t\tdesc = [w for w in desc if w.isalpha()]\n",
        "\t\t\tdesc = [w.translate(lemma) for w in desc]\n",
        "\t\t\tdesc_list[word] =  ' '.join(desc)\n",
        "\n",
        "\n",
        "def save_desc(text, name):\n",
        "\trow = []\n",
        "\tfor k, txt in text.items():\n",
        "\t\tfor word in txt:\n",
        "\t\t\trow.append(k + ' ' + word)\n",
        "\tlemma = '\\n'.join(row)\n",
        "\tf = open(name, 'w')\n",
        "\tf.write(lemma)\n",
        "\tf.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q6qlmu0ejqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenfile = 'Flickr8k_text/Flickr8k.token.txt'\n",
        "# loading token file\n",
        "tokens = load_file(tokenfile)\n",
        "# extracting descriptions\n",
        "descriptions = extract_descriptions(tokens)\n",
        "print('Loaded Descriptions:', len(descriptions))\n",
        "# processing\n",
        "proc_descriptions(descriptions)\n",
        "# creating vocabulary\n",
        "vocab = get_vocab(descriptions)\n",
        "print('Size of Vocabulary:', len(vocab))\n",
        "# saving descriptions file\n",
        "save_desc(descriptions, 'descriptions.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}